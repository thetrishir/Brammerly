{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version\t            Python version\t   Compiler\t    Build tools\t  cuDNN\t  CUDA\n",
    "# # tensorflow_gpu-2.10.0\t   3.7-3.10\t       MSVC 2019\tBazel 5.1.1\t   8.1\t  11.2\n",
    "\n",
    "# # %pip install nvidia-pyindex\n",
    "# # %pip install tensorflow-gpu==2.10.0\n",
    "# # %pip install torch\n",
    "# # %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# # %pip install transformers\n",
    "# # %pip install transformers[torch]\n",
    "# # %pip install accelerate>=0.26.0\n",
    "# %pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(\"PyTorch Version:\", torch.__version__)\n",
    "# print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"datasets/bert_data.csv\")  # Ensure your dataset is in the correct CSV format\n",
    "\n",
    "# Safely convert stringified lists back to Python lists\n",
    "df['attention_mask'] = df['attention_mask'].apply(ast.literal_eval)\n",
    "small_df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thetr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# 1. Split the data into training and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(small_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "# 3. Custom Dataset for BERT\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=64):\n",
    "        self.sentences = df['sentence'].tolist()\n",
    "        self.labels = df['label'].tolist()  # Add label support\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]  # Extract label\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(       # Tokenize and encode the sentence\n",
    "            sentence,\n",
    "            add_special_tokens=True,      # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_length,   # Set maximum length\n",
    "            padding='max_length',         # Pad sequences to max_length\n",
    "            truncation=True,              # Truncate sequences if needed\n",
    "            return_attention_mask=True,   # Return attention mask\n",
    "            return_tensors='pt'           # Return PyTorch tensors\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),           # Token IDs\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0), # Attention mask\n",
    "            'labels': torch.tensor(label, dtype=torch.long)          # Label as tensor\n",
    "        }\n",
    "\n",
    "# 4. Prepare datasets\n",
    "train_dataset = BertDataset(train_df, tokenizer)\n",
    "val_dataset = BertDataset(val_df, tokenizer)\n",
    "\n",
    "# Example DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Inspect the batch\n",
    "# for batch in train_loader:\n",
    "#     for k, v in batch.items():\n",
    "#         print(f\"{k}: {v.shape}\")  # Expected: [batch_size, seq_length]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\thetr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\thetr\\AppData\\Local\\Temp\\ipykernel_15356\\2007751912.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  3%|▎         | 10/300 [00:35<14:37,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6733, 'grad_norm': 7.260274410247803, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/300 [01:05<14:44,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6703, 'grad_norm': 5.957991600036621, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 30/300 [01:37<13:55,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.631, 'grad_norm': 7.557470798492432, 'learning_rate': 3e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [02:07<13:14,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5709, 'grad_norm': 6.978739261627197, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50/300 [02:39<12:58,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5008, 'grad_norm': 4.75669527053833, 'learning_rate': 5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60/300 [03:11<12:44,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4174, 'grad_norm': 5.441647052764893, 'learning_rate': 6e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 70/300 [03:42<11:52,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3493, 'grad_norm': 15.724018096923828, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80/300 [04:13<11:20,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3972, 'grad_norm': 4.687841892242432, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 90/300 [04:44<10:49,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.257, 'grad_norm': 3.990324020385742, 'learning_rate': 9e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100/300 [05:15<10:20,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1922, 'grad_norm': 9.479243278503418, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 33%|███▎      | 100/300 [05:30<10:20,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1646597683429718, 'eval_runtime': 15.271, 'eval_samples_per_second': 13.097, 'eval_steps_per_second': 1.637, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-100\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      " 37%|███▋      | 110/300 [06:24<11:16,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.116, 'grad_norm': 5.357348918914795, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 120/300 [06:55<09:23,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2777, 'grad_norm': 14.274627685546875, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 130/300 [07:27<08:54,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1875, 'grad_norm': 17.907670974731445, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 140/300 [07:58<08:19,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1702, 'grad_norm': 0.2626342177391052, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 150/300 [08:31<08:04,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2557, 'grad_norm': 3.4803872108459473, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 160/300 [09:05<07:54,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1029, 'grad_norm': 33.490211486816406, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 170/300 [09:40<07:26,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3043, 'grad_norm': 68.2522964477539, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 180/300 [10:17<07:10,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1937, 'grad_norm': 5.008024215698242, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 190/300 [10:52<06:51,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3404, 'grad_norm': 35.58915328979492, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [11:27<05:39,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0512, 'grad_norm': 30.20867347717285, 'learning_rate': 2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 200/300 [11:44<05:39,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13203367590904236, 'eval_runtime': 16.4823, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 1.517, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-200\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      " 70%|███████   | 210/300 [16:01<09:25,  6.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0945, 'grad_norm': 0.0517057329416275, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 220/300 [16:36<04:37,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1235, 'grad_norm': 0.06930798292160034, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 230/300 [17:11<03:57,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'grad_norm': 0.02909289486706257, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 240/300 [17:45<03:22,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1196, 'grad_norm': 0.0242856964468956, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 250/300 [18:19<02:49,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1721, 'grad_norm': 0.015851564705371857, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 260/300 [18:54<02:15,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0285, 'grad_norm': 0.02523624338209629, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 270/300 [19:29<01:46,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0254, 'grad_norm': 0.01997709833085537, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 280/300 [20:04<01:08,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.12, 'grad_norm': 0.12705987691879272, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 290/300 [20:38<00:34,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0777, 'grad_norm': 21.48626708984375, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [21:13<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1687, 'grad_norm': 0.11523807048797607, 'learning_rate': 3e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "                                                 \n",
      "100%|██████████| 300/300 [28:04<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13677270710468292, 'eval_runtime': 15.673, 'eval_samples_per_second': 12.761, 'eval_steps_per_second': 1.595, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "Saving vocabulary to ./results\\checkpoint-300\\vocab.txt: vocabulary indices are not consecutive. Please check that the vocabulary is not corrupted!\n",
      "100%|██████████| 300/300 [32:24<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1944.4213, 'train_samples_per_second': 1.234, 'train_steps_per_second': 0.154, 'train_loss': 0.25302942625557384, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [32:25<00:00,  6.48s/it]\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13203367590904236, 'eval_runtime': 14.7981, 'eval_samples_per_second': 13.515, 'eval_steps_per_second': 1.689, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"sagorsarker/bangla-bert-base\", num_labels=2)\n",
    "\n",
    "# 6. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory for model and logs\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save the model after every epoch\n",
    "    load_best_model_at_end=True      # Load the best model at the end\n",
    ")\n",
    "\n",
    "# 7. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                     # Model to train\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=train_dataset,     # Training dataset\n",
    "    eval_dataset=val_dataset,        # Validation dataset\n",
    "    tokenizer=tokenizer              # Tokenizer used to process input\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.13203367590904236, 'eval_runtime': 14.9007, 'eval_samples_per_second': 13.422, 'eval_steps_per_second': 1.678, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: গাজীপুরের কালিয়াকৈর উপজেলার তেলিরচালা এলাকায়\n",
      "Prediction: Correct\n",
      "\n",
      "Sentence: গাজীপ৳ুরের ালিয়াকৈর উপজেলংার তেলিচরালা এলাকা়\n",
      "Prediction: Incorrect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample new sentences for inference\n",
    "new_sentences = [\n",
    "    \"গাজীপুরের কালিয়াকৈর উপজেলার তেলিরচালা এলাকায়\",  # Correct sentence\n",
    "    \"গাজীপ৳ুরের ালিয়াকৈর উপজেলংার তেলিচরালা এলাকা়\"  # Sentence with errors\n",
    "]\n",
    "\n",
    "# Preprocess and tokenize new sentences\n",
    "inputs = tokenizer(new_sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Move inputs to model's device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "# Interpret predictions\n",
    "for sentence, pred in zip(new_sentences, predictions):\n",
    "    label = \"Correct\" if pred == 0 else \"Incorrect\"\n",
    "    print(f\"Sentence: {sentence}\\nPrediction: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./brammerly\")\n",
    "tokenizer.save_pretrained(\"./brammerly\")\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./brammerly\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./brammerly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "# # Debug shapes\n",
    "# print(\"Input IDs shape:\", batch['input_ids'].shape)\n",
    "# print(\"Attention Mask shape:\", batch['attention_mask'].shape)\n",
    "\n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "#     logits = outputs.logits\n",
    "#     preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# print(\"Predictions:\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create predictions for validation data\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in val_dataset:  # val_dataloader is your DataLoader for validation data\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "    true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# Convert to DataFrame for easy inspection\n",
    "val_df['predicted_label'] = predictions\n",
    "val_df['true_label'] = true_labels\n",
    "\n",
    "# Inspect misclassified examples\n",
    "misclassified = val_df[val_df['predicted_label'] != val_df['true_label']]\n",
    "print(\"Misclassified Examples:\")\n",
    "print(misclassified[['sentence', 'true_label', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Correct\", \"Incorrect\"], yticklabels=[\"Correct\", \"Incorrect\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Accuracy, Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels, predictions, target_names=[\"Correct\", \"Incorrect\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GECToR dependencies (done once)\n",
    "# !pip install allennlp allennlp-models\n",
    "\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp_models.pretrained import load_predictor\n",
    "\n",
    "# Load GECToR pre-trained model\n",
    "predictor = load_predictor(\"https://storage.googleapis.com/allennlp-public-models/roberta-base-2020.06.09.tar.gz\")\n",
    "\n",
    "# Example prediction\n",
    "example_sentence = \"তোমরা ভুলগুলো চিহ্নিত করনি।\"\n",
    "prediction = predictor.predict(sentence=example_sentence)\n",
    "\n",
    "# Output\n",
    "print(\"Original:\", example_sentence)\n",
    "print(\"Corrected:\", prediction[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GECToR Fine-Tuning for Bangla Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GECToR Repo\n",
    "git clone https://github.com/grammarly/gector.git\n",
    "cd gector\n",
    "\n",
    "# Install Dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Train GECToR\n",
    "python train.py \\\n",
    "    --train_set \"data/train.txt\" \\\n",
    "    --dev_set \"data/dev.txt\" \\\n",
    "    --model_dir \"output/bangla_model\" \\\n",
    "    --pretrained_transformer \"sagorsarker/bangla-bert-base\" \\\n",
    "    --vocab_path \"data/vocab\" \\\n",
    "    --batch_size 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune T5 for Sentence-Level Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Replace with Bangla-supported T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Prepare data\n",
    "train_data = {\"input_text\": df['synth_spelling_errors'].tolist(), \n",
    "              \"target_text\": df['cleaned_text'].tolist()}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"correct: \" + text for text in examples['input_text']]\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_bangla_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./t5_bangla_finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 3: Sentence Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"google/mt5-small\"  # Use T5 pre-trained model for multilingual tasks\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize data for T5\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"grammar: {i}\" for i in examples[\"incorrect_sentence\"]]\n",
    "    outputs = examples[\"cleaned_text\"]\n",
    "    model_inputs = tokenizer(inputs, text_target=outputs, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize train and validation sets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"t5_sentence_correction_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate GECToR, T5, and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_pipeline(sentence, gector_model, t5_model, bert_model, tokenizer):\n",
    "    # Step 1: GECToR Token Correction\n",
    "    gector_corrected_sentence = gector_model.correct(sentence)\n",
    "\n",
    "    # Step 2: T5 Sentence-Level Correction\n",
    "    input_text = \"correct: \" + gector_corrected_sentence\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    t5_outputs = t5_model.generate(**inputs)\n",
    "    t5_corrected_sentence = tokenizer.decode(t5_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Step 3: BERT Error Verification\n",
    "    inputs = tokenizer(t5_corrected_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = bert_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return {\"original\": sentence, \n",
    "            \"gector\": gector_corrected_sentence, \n",
    "            \"t5\": t5_corrected_sentence, \n",
    "            \"bert_label\": \"Correct\" if prediction.item() == 1 else \"Incorrect\"}\n",
    "\n",
    "# Example usage\n",
    "sentence = \"গাজীপ৳ুরের ালিয়াকৈর উপজেলংার তেলিচরালা এলাকা়...\"\n",
    "result = correction_pipeline(sentence, gector_model, t5_model, bert_model, tokenizer)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
